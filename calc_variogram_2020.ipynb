{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5fbaf4-765a-4665-9980-f8b6f41187e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:24.077853Z",
     "iopub.status.busy": "2024-01-23T12:19:24.077376Z",
     "iopub.status.idle": "2024-01-23T12:19:27.636299Z",
     "shell.execute_reply": "2024-01-23T12:19:27.634975Z",
     "shell.execute_reply.started": "2024-01-23T12:19:24.077807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the CPRS for the ICPAC region forecasts\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors  # For consistency with Harris et. al 2022\n",
    "from datetime import datetime, timedelta\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import multivariate_normal\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7097311d-f7b8-41bf-89b4-51cea903a11c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:27.639459Z",
     "iopub.status.busy": "2024-01-23T12:19:27.638753Z",
     "iopub.status.idle": "2024-01-23T12:19:27.665864Z",
     "shell.execute_reply": "2024-01-23T12:19:27.665238Z",
     "shell.execute_reply.started": "2024-01-23T12:19:27.639411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default function to returns the weights used to score two points in an image.\n",
    "# This function sets how important neighbouring points are considered to be in variogram_score.\n",
    "#\n",
    "# In the estimate of the expectation values, the difference between two points often increases with\n",
    "# distance because their correlation decreases. In the estimate of the expectation values, the\n",
    "# difference between two points often increases with distance because their correlation decreases.\n",
    "# Downweighting pairs that are expected to have relatively weak correlations can therefore benifit\n",
    "# the signal-to-noise ratio.\n",
    "#\n",
    "# Arguments:\n",
    "#   x1 - Integer horizontal coordinate of point 1 in pixels.\n",
    "#   y1 - Integer vertical   coordinate of point 1 in pixels.\n",
    "#   x2 - Integer horizontal coordinate of point 2 in pixels.\n",
    "#   y2 - Integer vertical   coordinate of point 2 in pixels.\n",
    "# Returns:\n",
    "#   weight  - The weight for the variogram_score for this pair of pixels.\n",
    "#\n",
    "def weights_function(x1, y1, x2, y2):\n",
    "    \n",
    "    # Exponential decay constant in units of pixels\n",
    "    decay_constant = -1\n",
    "    \n",
    "    distance = np.sqrt((x1-x2)**2 + (y1-y2)**2)\n",
    "    weight = np.exp(decay_constant * distance)\n",
    "    return weight\n",
    "\n",
    "# Return unit weights for all point combinations\n",
    "def unit_weights(x1, y1, x2, y2):\n",
    "    return 1.0\n",
    "\n",
    "# Computes a variogram score image for ne member ensemble forecasts of measurement images of size nx by ny\n",
    "#Â pixels averaged over nt images.\n",
    "# Arguments:\n",
    "#   measurements_images                 - An array of size (nt, nx, ny) or (nx, ny) or (ny) measurements.\n",
    "#   forecast_ensemble_images            - An array of ensemble forecasts of the measurements of size\n",
    "#                                         (nt, ne, nx, ny) or (ne, nx, ny) or (ne, ny) or (ny).\n",
    "#   p = 0.5                             - The moment parameter of the score. \n",
    "#   weights_function = weights_function - A user defined function to return the weights for arbitrary pairs of\n",
    "#                                         pixels. See the definition of the default weights_function above.\n",
    "#   D_max = 5                           - The maximum distance in pixels at which to compare points in the score.\n",
    "# Returns:\n",
    "#   The contribution to the variogram score for each point. Has the same dimensions (nx,ny) as a single \n",
    "#   measurements image. To get the variogram score, average over each pixel.\n",
    "#\n",
    "def variogram_score_slow(measurements_images,\n",
    "                    forecast_ensemble_images,\n",
    "                    p=0.5,\n",
    "                    weights_function=weights_function,\n",
    "                    D_max=5):\n",
    "    \n",
    "    # Extract nt, ne, nx, ny and do basic parameter checks\n",
    "    \n",
    "    if (forecast_ensemble_images.ndim == 1):\n",
    "        # A single vector\n",
    "        nt = 1\n",
    "        ne = 1\n",
    "        nx = 1\n",
    "        (ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        ntm = 1\n",
    "        nxm = 1\n",
    "        (nym) = measurements_images.shape\n",
    "    \n",
    "    elif (forecast_ensemble_images.ndim == 2):\n",
    "        # An ensemble of vectors\n",
    "        nt = 1\n",
    "        nx = 1\n",
    "        (ne,ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        ntm = 1\n",
    "        nxm = 1\n",
    "        (nym) = measurements_images.shape\n",
    "    \n",
    "    elif (forecast_ensemble_images.ndim == 3):\n",
    "        # An ensemble of images\n",
    "        nt = 1\n",
    "        (ne, nx, ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        ntm = 1\n",
    "        (nxm,nym) = measurements_images.shape\n",
    "        \n",
    "    elif (forecast_ensemble_images.ndim == 4):\n",
    "        # nt image ensembles\n",
    "        (nt, ne, nx, ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        (ntm,nxm,nym) = measurements_images.shape\n",
    "        \n",
    "    else:\n",
    "        raise Exception(f\"forecast_ensemble_images must be an array with 1, 2, 3 or 4 dimensions but it has {forecast_ensemble_images.ndim}.\")\n",
    "    \n",
    "    if (nt != ntm):\n",
    "        raise Exception(f\"Number of forecasts ({nt}) does not equal the number of measurement fields ({ntm}).\")\n",
    "    if (nx != nxm):\n",
    "        raise Exception(f\"Number of forecast pixels (nx = {nx}) does not equal the number of measurement pixels (nx = {nxm}).\")\n",
    "    if (ny != nym):\n",
    "        raise Exception(f\"Number of forecast pixels (ny = {ny}) does not equal the number of measurement pixels (ny = {nym}).\")\n",
    "    if (nx*ny < 2):\n",
    "        raise Exception(f\"At least two variables are required for the variogram score but nx*ny = {nx*ny}.\")\n",
    "    if (p <= 0.0):\n",
    "        raise Exception(\"p must be > 0\")\n",
    "    if (D_max < 1):\n",
    "        raise Exception(\"D_max must be >= 1\")\n",
    "    \n",
    "    if (forecast_ensemble_images.ndim < 4):\n",
    "        # Make arrays a standard shape (RAM inefficient)\n",
    "        forecast_ensemble_images = np.reshape(forecast_ensemble_images, (nt,ne,nx,ny))\n",
    "        measurements_images = np.reshape(measurements_images, (ntm, nxm, nym))\n",
    "    \n",
    "    # The variogram score image\n",
    "    score = np.zeros((nx, ny))\n",
    "    count = np.zeros((nx, ny), dtype=int)\n",
    "    \n",
    "    # Where there are no measurements at all, mask the score\n",
    "    mask = np.sum(np.isnan(measurements_images), axis=0) != nt\n",
    "    \n",
    "    # For each measurement time\n",
    "    for k in range(nt):\n",
    "    \n",
    "        # For each point in the image\n",
    "        for i1 in range(nx):\n",
    "            for j1 in range(ny):\n",
    "                \n",
    "                # No point computing anything for this measurement if it is np.nan\n",
    "                if not np.isnan(measurements_images[k,i1,j1]):\n",
    "                    \n",
    "                    # Loop over the points surrounding this one\n",
    "                    pixel_score = 0  # The current score at this pixel\n",
    "                    score_invalid = False  # The score is valid so far\n",
    "                    for i2 in range(np.max([0,i1-D_max]), np.min([nx,i1+D_max+1])):\n",
    "                        for j2 in range(np.max([0,j1-D_max]), np.min([ny,j1+D_max+1])):\n",
    "                            \n",
    "                            # If the point is within the cut-off radius and is not the point we are considering\n",
    "                            # and it is not masked out for all time.\n",
    "                            dist = np.sqrt((i1-i2)**2 + (j1-j2)**2)\n",
    "                            if (dist <= D_max) and (dist > 0.0) and (mask[i2,j2]):\n",
    "\n",
    "                                # Can only be here if the mask[i2,j2] is True, so this is a temporary missing measurement\n",
    "                                if np.isnan(measurements_images[k,i2,j2]):\n",
    "                                    score_invalid = True\n",
    "                                    break\n",
    "                                \n",
    "                                # Estimate expectation values\n",
    "                                E = np.mean(np.abs(forecast_ensemble_images[k,:,i1,j1] - forecast_ensemble_images[k,:,i2,j2])**0.5)\n",
    "\n",
    "                                # Compute the measurement difference\n",
    "                                dy = np.abs(measurements_images[k,i1,j1] - measurements_images[k,i2,j2])**p\n",
    "\n",
    "                                # Compute the weights\n",
    "                                w = weights_function(i1,j1,i2,j2)\n",
    "\n",
    "                                # Compute the score for this pixel\n",
    "                                pixel_score += w * (dy - E)**2\n",
    "                                \n",
    "                        if score_invalid:\n",
    "                            break\n",
    "                    \n",
    "                    if not score_invalid:\n",
    "                        score[i1,j1] += pixel_score\n",
    "                        # Keep track of the number of measurements used at this pixel\n",
    "                        count[i1,j1] += 1\n",
    "    \n",
    "    # Average over the number of measurement times\n",
    "    score /= (count + (count == 0))  # Take into account counts of 0\n",
    "    \n",
    "    # Don't return a score when no score was computed\n",
    "    score[count == 0] = np.nan\n",
    "    \n",
    "    return score\n",
    "\n",
    "# More optimised version of variogram_score_slow above\n",
    "def variogram_score(measurements_images,\n",
    "                    forecast_ensemble_images,\n",
    "                    p=0.5,\n",
    "                    weights_function=weights_function,\n",
    "                    D_max=5):\n",
    "    \n",
    "    # Extract nt, ne, nx, ny and do basic parameter checks\n",
    "    \n",
    "    if (forecast_ensemble_images.ndim == 1):\n",
    "        # A single vector\n",
    "        nt = 1\n",
    "        ne = 1\n",
    "        nx = 1\n",
    "        (ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        ntm = 1\n",
    "        nxm = 1\n",
    "        (nym) = measurements_images.shape\n",
    "    \n",
    "    elif (forecast_ensemble_images.ndim == 2):\n",
    "        # An ensemble of vectors\n",
    "        nt = 1\n",
    "        nx = 1\n",
    "        (ne,ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        ntm = 1\n",
    "        nxm = 1\n",
    "        (nym) = measurements_images.shape\n",
    "    \n",
    "    elif (forecast_ensemble_images.ndim == 3):\n",
    "        # An ensemble of images\n",
    "        nt = 1\n",
    "        (ne, nx, ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        ntm = 1\n",
    "        (nxm,nym) = measurements_images.shape\n",
    "        \n",
    "    elif (forecast_ensemble_images.ndim == 4):\n",
    "        # nt image ensembles\n",
    "        (nt, ne, nx, ny) = forecast_ensemble_images.shape\n",
    "        \n",
    "        (ntm,nxm,nym) = measurements_images.shape\n",
    "        \n",
    "    else:\n",
    "        raise Exception(f\"forecast_ensemble_images must be an array with 1, 2, 3 or 4 dimensions but it has {forecast_ensemble_images.ndim}.\")\n",
    "    \n",
    "    if (nt != ntm):\n",
    "        raise Exception(f\"Number of forecasts ({nt}) does not equal the number of measurement fields ({ntm}).\")\n",
    "    if (nx != nxm):\n",
    "        raise Exception(f\"Number of forecast pixels (nx = {nx}) does not equal the number of measurement pixels (nx = {nxm}).\")\n",
    "    if (ny != nym):\n",
    "        raise Exception(f\"Number of forecast pixels (ny = {ny}) does not equal the number of measurement pixels (ny = {nym}).\")\n",
    "    if (nx*ny < 2):\n",
    "        raise Exception(f\"At least two variables are required for the variogram score but nx*ny = {nx*ny}.\")\n",
    "    if (p <= 0.0):\n",
    "        raise Exception(\"p must be > 0\")\n",
    "    if (D_max < 1):\n",
    "        raise Exception(\"D_max must be >= 1\")\n",
    "    \n",
    "    if (forecast_ensemble_images.ndim < 4):\n",
    "        # Make arrays a standard shape (RAM inefficient)\n",
    "        forecast_ensemble_images = np.reshape(forecast_ensemble_images, (nt,ne,nx,ny))\n",
    "        measurements_images = np.reshape(measurements_images, (ntm, nxm, nym))\n",
    "    \n",
    "    # Precompute weights\n",
    "    i1 = 0\n",
    "    j1 = 0\n",
    "    w = np.zeros((2*D_max+1,2*D_max+1))\n",
    "    for i2 in range(i1-D_max, i1+D_max+1):\n",
    "        for j2 in range(j1-D_max, j1+D_max+1):\n",
    "            dist = np.sqrt((i1-i2)**2 + (j1-j2)**2)\n",
    "            if (dist <= D_max) and (dist > 0.0):\n",
    "                w[i2+D_max,j2+D_max] = weights_function(i1,j1,i2,j2)\n",
    "            else:  # Out of range\n",
    "                w[i2+D_max,j2+D_max] = 0\n",
    "    \n",
    "    # The variogram score image\n",
    "    score = np.zeros((nx, ny))\n",
    "    count = np.zeros((nx, ny), dtype=int)\n",
    "    \n",
    "    # Where there are no measurements at all, mask the score\n",
    "    mask = np.sum(np.isnan(measurements_images), axis=0) != nt\n",
    "    \n",
    "    # For each measurement time\n",
    "    for k in range(nt):\n",
    "    \n",
    "        # For each point in the image\n",
    "        for i1 in range(nx):\n",
    "            for j1 in range(ny):\n",
    "                \n",
    "                # No point computing anything for this measurement if it is np.nan\n",
    "                if not np.isnan(measurements_images[k,i1,j1]):\n",
    "                \n",
    "                    i2_start = np.max([0,i1-D_max])\n",
    "                    i2_end = np.min([nx,i1+D_max+1])\n",
    "\n",
    "                    j2_start = np.max([0,j1-D_max])\n",
    "                    j2_end = np.min([ny,j1+D_max+1])\n",
    "\n",
    "                    # Compute the measurement difference\n",
    "                    dy = np.abs(measurements_images[k,i1,j1] - measurements_images[k,i2_start:i2_end,j2_start:j2_end])**0.5\n",
    "                    \n",
    "                    # The weights for this specific i1 and j1\n",
    "                    wij = w[i2_start-i1+D_max:i2_end-i1+D_max,j2_start-j1+D_max:j2_end-j1+D_max]\n",
    "                    \n",
    "                    # The valid numbers in dy are np.isnan(dy)                    \n",
    "                    # The valid numbers in the mask are np.logical_not(mask[i2_start:i2_end,j2_start:j2_end])\n",
    "                    # We don't care if the number is valid or not if the weight is zero\n",
    "                    # If there is an extra np.nan then reject this measurement (XXX Ugly can I just use masked arrays?)\n",
    "                    if (np.sum((wij > 0) & (np.isnan(dy) ^ np.logical_not(mask[i2_start:i2_end,j2_start:j2_end]))) == 0):\n",
    "                        \n",
    "                        # Estimate expectation values in the mask region\n",
    "                        E = np.abs(forecast_ensemble_images[k,0,i1,j1] - forecast_ensemble_images[k,0,i2_start:i2_end,j2_start:j2_end])**0.5\n",
    "                        for m in range(1,ne):\n",
    "                            E += np.abs(forecast_ensemble_images[k,m,i1,j1] - forecast_ensemble_images[k,m,i2_start:i2_end,j2_start:j2_end])**0.5\n",
    "                        E /= ne\n",
    "                        \n",
    "                        # Compute the score for this pixel\n",
    "                        pixel_score = np.nansum(wij * (dy - E)**2)\n",
    "\n",
    "                        score[i1,j1] += pixel_score\n",
    "                        # Keep track of the number of measurements used at this pixel\n",
    "                        count[i1,j1] += 1\n",
    "                    \n",
    "    # Average over the number of measurement times\n",
    "    score /= (count + (count == 0))  # Take into account counts of 0\n",
    "        \n",
    "    # Don't return a score when no score was computed\n",
    "    score[count == 0] = np.nan\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330c855d-055a-4d3f-b48f-54dbc3dc77db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:27.666565Z",
     "iopub.status.busy": "2024-01-23T12:19:27.666429Z",
     "iopub.status.idle": "2024-01-23T12:19:27.684289Z",
     "shell.execute_reply": "2024-01-23T12:19:27.683229Z",
     "shell.execute_reply.started": "2024-01-23T12:19:27.666552Z"
    }
   },
   "outputs": [],
   "source": [
    "year = 2020\n",
    "\n",
    "IMERG_data_dir = \"/home/c/cooperf/data/IMERG_6h\"\n",
    "# This is Andrew's quick botch on the data so don't trust it\n",
    "IFS_data_dir = f\"/home/c/cooperf/IFS/IFS-regICPAC-ens-tp\"\n",
    "plot_dir = \"/home/c/cooperf/IFS/junk_testing_data/pictures\"\n",
    "save_plots = False\n",
    "\n",
    "# To be consistent with the Harris et. al paper.\n",
    "value_range_precip = (0.1, 15)\n",
    "\n",
    "# Load the constant in time latitude and longitude from a random file\n",
    "# file_name = IMERG_data_dir + \"/3B-HHR.MS.MRG.3IMERG.20210930-S233000-E235959.1410.V06B.HDF5.nc4\"\n",
    "file_name = f\"{IMERG_data_dir}/20200101_00.nc4\"\n",
    "nc_file = nc.Dataset(file_name)\n",
    "latitude = np.array(nc_file[\"lat\"][:])  # Cropping the first point for consistency with the IFS region\n",
    "longitude = np.array(nc_file[\"lon\"][:])\n",
    "nc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf65bac-d384-4841-88c6-2dddbdc39a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:27.685626Z",
     "iopub.status.busy": "2024-01-23T12:19:27.685390Z",
     "iopub.status.idle": "2024-01-23T12:19:27.701606Z",
     "shell.execute_reply": "2024-01-23T12:19:27.700653Z",
     "shell.execute_reply.started": "2024-01-23T12:19:27.685613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load example IMERG rainfall\n",
    "d = datetime(year,1,13,0)\n",
    "count = 360  # There is an additional number in the file name\n",
    "# file_name = f\"{IMERG_data_dir}/3B-HHR.MS.MRG.3IMERG.{year}{d.month:02d}{d.day:02d}-S{d.hour:02d}{d.minute:02d}00-E{d.hour:02d}{d.minute+29:02d}59.{count:04d}.V06B.HDF5.nc4\"\n",
    "file_name = f\"{IMERG_data_dir}/{year}{d.month:02d}{d.day:02d}_{d.hour:02d}.nc4\"\n",
    "nc_file = nc.Dataset(file_name)\n",
    "IMERG_rain = np.array(nc_file[\"precipitationCal\"][:,:])\n",
    "nc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad32f077-558c-4c6e-83f7-88e6b0ef74a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:27.702323Z",
     "iopub.status.busy": "2024-01-23T12:19:27.702192Z",
     "iopub.status.idle": "2024-01-23T12:19:28.553085Z",
     "shell.execute_reply": "2024-01-23T12:19:28.552131Z",
     "shell.execute_reply.started": "2024-01-23T12:19:27.702310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load example IFS rainfall with a 24 hour lead time\n",
    "file_name = f\"{IFS_data_dir}/{year}/tp_1.nc\"\n",
    "nc_file = nc.Dataset(file_name)\n",
    "forecast_times = np.array(nc_file[\"time\"][:])\n",
    "valid_times = np.array(nc_file[\"valid_time\"][:])\n",
    "tp_IFS_accum = np.array(nc_file[\"tp\"][12,:,4:6,1:,:])*1000/6  # Convert from m/6h to mm/h\n",
    "tp_IFS = tp_IFS_accum[:,1,:,:] - tp_IFS_accum[:,0,:,:]\n",
    "nc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e93286-37ee-4c04-b4ff-93a9462e3506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:28.554363Z",
     "iopub.status.busy": "2024-01-23T12:19:28.554027Z",
     "iopub.status.idle": "2024-01-23T12:19:28.700070Z",
     "shell.execute_reply": "2024-01-23T12:19:28.699194Z",
     "shell.execute_reply.started": "2024-01-23T12:19:28.554342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load example cGAN rainfall with a 24 hour lead time\n",
    "file_name = f\"/home/c/cooperf/data/cGAN/ICPAC/GAN_forecasts/GAN_12.nc\"\n",
    "nc_file = nc.Dataset(file_name)\n",
    "forecast_times_GAN = np.array(nc_file[\"time\"][:])\n",
    "valid_times_GAN = np.array(nc_file[\"fcst_valid_time\"][:])\n",
    "tp_GAN = np.array(nc_file[\"precipitation\"][0,:,0,:,:])\n",
    "nc_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75501b07-5c58-499f-be54-0f35d3e83120",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:28.701057Z",
     "iopub.status.busy": "2024-01-23T12:19:28.700785Z",
     "iopub.status.idle": "2024-01-23T12:19:28.705203Z",
     "shell.execute_reply": "2024-01-23T12:19:28.704554Z",
     "shell.execute_reply.started": "2024-01-23T12:19:28.701042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 352)\n",
      "(50, 384, 352)\n",
      "(50, 384, 352)\n"
     ]
    }
   ],
   "source": [
    "print(IMERG_rain.shape)\n",
    "print(tp_IFS.shape)\n",
    "print(tp_GAN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5256a504-4503-4eb8-b7ff-9073f95b2f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T12:19:28.705859Z",
     "iopub.status.busy": "2024-01-23T12:19:28.705724Z",
     "iopub.status.idle": "2024-01-23T12:20:30.258899Z",
     "shell.execute_reply": "2024-01-23T12:20:30.256487Z",
     "shell.execute_reply.started": "2024-01-23T12:19:28.705845Z"
    }
   },
   "outputs": [],
   "source": [
    "IFS_variogram = variogram_score(IMERG_rain, tp_IFS)\n",
    "cGAN_variogram = variogram_score(IMERG_rain, tp_GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d7078-71b5-479e-baa4-374f0d189c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T18:53:55.938813Z",
     "iopub.status.busy": "2024-01-23T18:53:55.938214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "# Now compare all of the GAN forecasts, IFS forecasts and IMERG data\n",
    "\n",
    "# IFS_variogram_all = np.zeros((366,4,len(latitude),len(longitude)))\n",
    "# cGAN_variogram_all = np.zeros((366,4,len(latitude),len(longitude)))\n",
    "# forecast_times = np.zeros((366,4))\n",
    "# valid_times = np.zeros((366,4))\n",
    "# forecast_times_GAN = np.zeros((366,4))\n",
    "# valid_times_GAN = np.zeros((366,4))\n",
    "\n",
    "# XXX Broke so restarted\n",
    "for day_num in range(43,366):   # 0 to 365 (366 days in 2020)\n",
    "    print(day_num)\n",
    "    for valid_time_num in range(4):  # There are 4 valid times in each GAN forecast\n",
    "\n",
    "        # Load example IMERG rainfall\n",
    "        d = datetime(year,1,2,6*valid_time_num) + timedelta(days=day_num)\n",
    "        file_name = f\"{IMERG_data_dir}/{year}{d.month:02d}{d.day:02d}_{d.hour:02d}.nc4\"\n",
    "        nc_file = nc.Dataset(file_name)\n",
    "        IMERG_rain = np.array(nc_file[\"precipitationCal\"][:,:])\n",
    "        nc_file.close()\n",
    "        \n",
    "        # Load example IFS rainfall with a 24 hour lead time\n",
    "        forecast_start_date = d-timedelta(days=1)  # 24 hour lead time\n",
    "        file_name = f\"{IFS_data_dir}/{year}/tp_{forecast_start_date.month}.nc\"\n",
    "        nc_file = nc.Dataset(file_name)\n",
    "        forecast_time = np.array(nc_file[\"time\"][forecast_start_date.day-1])\n",
    "        valid_time = np.array(nc_file[\"valid_time\"][forecast_start_date.day-1,4+valid_time_num])\n",
    "        tp_IFS_accum = np.array(nc_file[\"tp\"][forecast_start_date.day-1,:,4+valid_time_num:6+valid_time_num,1:,:])*1000/6  # Convert from m/6h to mm/h\n",
    "        tp_IFS = tp_IFS_accum[:,1,:,:] - tp_IFS_accum[:,0,:,:]\n",
    "        nc_file.close()\n",
    "        \n",
    "        # Load the cGAN forecast\n",
    "        file_name = f\"/home/c/cooperf/data/cGAN/ICPAC/GAN_forecasts/GAN_{day_num}.nc\"\n",
    "        nc_file = nc.Dataset(file_name)\n",
    "        forecast_time_GAN = np.array(nc_file[\"time\"][0])\n",
    "        valid_time_GAN = np.array(nc_file[\"fcst_valid_time\"][0,valid_time_num])\n",
    "        tp_GAN = np.array(nc_file[\"precipitation\"][0,:,valid_time_num,:,:])\n",
    "        nc_file.close()\n",
    "\n",
    "        # Compute the variogram score\n",
    "        IFS_variogram = variogram_score(IMERG_rain, tp_IFS)\n",
    "        cGAN_variogram = variogram_score(IMERG_rain, tp_GAN)\n",
    "        \n",
    "        # Save for later\n",
    "        IFS_variogram_all[day_num,valid_time_num,:,:] = IFS_variogram\n",
    "        cGAN_variogram_all[day_num,valid_time_num,:,:] = cGAN_variogram\n",
    "        forecast_times[day_num,valid_time_num] = forecast_time\n",
    "        valid_times[day_num,valid_time_num] = valid_time\n",
    "        forecast_times_GAN[day_num,valid_time_num] = forecast_time_GAN\n",
    "        valid_times_GAN[day_num,valid_time_num] = valid_time_GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e8311e2-00c1-4228-9ca0-7e7447dc4067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T17:56:56.073393Z",
     "iopub.status.busy": "2024-01-30T17:56:56.072675Z",
     "iopub.status.idle": "2024-01-30T17:56:56.082456Z",
     "shell.execute_reply": "2024-01-30T17:56:56.081188Z",
     "shell.execute_reply.started": "2024-01-30T17:56:56.073345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b54d499-ea70-4fc3-9768-f5175b0ac8a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T17:56:42.798287Z",
     "iopub.status.busy": "2024-01-30T17:56:42.797656Z",
     "iopub.status.idle": "2024-01-30T17:56:42.809891Z",
     "shell.execute_reply": "2024-01-30T17:56:42.808677Z",
     "shell.execute_reply.started": "2024-01-30T17:56:42.798232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Check the valid times are the same\n",
    "print(np.max(np.abs(forecast_times - forecast_times_GAN)))\n",
    "print(np.max(np.abs(valid_times - valid_times_GAN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f5def2-61c8-4628-9f99-407f6e7a0a16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T17:57:01.873987Z",
     "iopub.status.busy": "2024-01-30T17:57:01.873343Z",
     "iopub.status.idle": "2024-01-30T17:57:40.241873Z",
     "shell.execute_reply": "2024-01-30T17:57:40.240869Z",
     "shell.execute_reply.started": "2024-01-30T17:57:01.873927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the data for later processing\n",
    "output_data_dir = \"/home/c/cooperf/IFS/To_ICPAC/Jan_2024\"\n",
    "file_name = \"variogram_2020.nc\"\n",
    "\n",
    "# Create a new NetCDF file\n",
    "rootgrp = nc.Dataset(f\"{output_data_dir}/{file_name}\", \"w\", format=\"NETCDF4\")\n",
    "\n",
    "# Describe where this data comes from\n",
    "rootgrp.description = \"IFS and cGAN variogram vs IMERG rain data for 2020.\"\n",
    "\n",
    "# Create dimensions\n",
    "longitude_dim = rootgrp.createDimension(\"longitude\", len(longitude))\n",
    "latitude_dim = rootgrp.createDimension(\"latitude\", len(latitude))\n",
    "time_dim = rootgrp.createDimension(\"time\", None)\n",
    "valid_time_dim = rootgrp.createDimension(\"valid_time\", 4)\n",
    "#ensemble_dim = rootgrp.createDimension(\"member\", num_ensemble_members)\n",
    "\n",
    "# Create the longitude variable\n",
    "longitude_data = rootgrp.createVariable(\"longitude\", \"f4\", (\"longitude\"), zlib=False)\n",
    "longitude_data.units = \"degrees_east\"\n",
    "longitude_data[:] = longitude   # Write the longitude data\n",
    "\n",
    "# Create the latitude variable\n",
    "latitude_data = rootgrp.createVariable(\"latitude\", \"f4\", (\"latitude\"), zlib=False)\n",
    "latitude_data.units = \"degrees_north\"\n",
    "latitude_data[:] = latitude     # Write the latitude data\n",
    "\n",
    "# Create the time variable\n",
    "time_data = rootgrp.createVariable(\"time\", \"f4\", (\"time\"), zlib=False)\n",
    "time_data.units = \"hours since 1900-01-01 00:00:00.0\"\n",
    "time_data[:] = forecast_times[:,0]     # Write the forecast_times data\n",
    "\n",
    "# Create the valid_time variable\n",
    "valid_time_data = rootgrp.createVariable(\"valid_time\", \"f4\", (\"time\",\"valid_time\"), zlib=False)\n",
    "valid_time_data.units = \"hours since 1900-01-01 00:00:00.0\"\n",
    "valid_time_data[:] = valid_times     # Write the valid_times data\n",
    "\n",
    "# Create the IFS_CRPS variable\n",
    "IFS_variogram_data = rootgrp.createVariable(\"IFS_variogram_score\", \"f4\", (\"time\",\"valid_time\",\"latitude\",\"longitude\"), zlib=True)\n",
    "IFS_variogram_data.description = \"IFS vs IMERG variogram score\"\n",
    "IFS_variogram_data.units = \"mm/h\"\n",
    "IFS_variogram_data[:] = IFS_variogram_all     # Write the IMERG_rain_all data\n",
    "\n",
    "# Create the IFS_CRPS variable\n",
    "cGAN_variogram_data = rootgrp.createVariable(\"cGAN_variogram_score\", \"f4\", (\"time\",\"valid_time\",\"latitude\",\"longitude\"), zlib=True)\n",
    "cGAN_variogram_data.description = \"cGAN vs IMERG variogram score\"\n",
    "cGAN_variogram_data.units = \"mm/h\"\n",
    "cGAN_variogram_data[:] = cGAN_variogram_all     # Write the IMERG_rain_all data\n",
    "\n",
    "# Close the NetCDF file\n",
    "rootgrp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268511dc-75d2-4710-8f16-5dace8ddf297",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-23T18:53:27.899943Z",
     "iopub.status.idle": "2024-01-23T18:53:27.900108Z",
     "shell.execute_reply": "2024-01-23T18:53:27.900032Z",
     "shell.execute_reply.started": "2024-01-23T18:53:27.900025Z"
    }
   },
   "outputs": [],
   "source": [
    "IFS_variogram_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b634283-393b-49ac-9339-cac8ab27c864",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-23T18:53:27.900846Z",
     "iopub.status.idle": "2024-01-23T18:53:27.901017Z",
     "shell.execute_reply": "2024-01-23T18:53:27.900942Z",
     "shell.execute_reply.started": "2024-01-23T18:53:27.900935Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(IFS_variogram_all[:,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ace7f-7901-4df8-903e-6e541c7bd581",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-23T18:53:27.901609Z",
     "iopub.status.idle": "2024-01-23T18:53:27.901771Z",
     "shell.execute_reply": "2024-01-23T18:53:27.901697Z",
     "shell.execute_reply.started": "2024-01-23T18:53:27.901690Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(cGAN_variogram_all[:,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9556252-95f5-48be-a46f-351a7a2efe03",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-23T18:53:27.902601Z",
     "iopub.status.idle": "2024-01-23T18:53:27.902769Z",
     "shell.execute_reply": "2024-01-23T18:53:27.902692Z",
     "shell.execute_reply.started": "2024-01-23T18:53:27.902684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the figure and each axis for the rows and columns\n",
    "fig, axs = plt.subplots(nrows=1,ncols=2,\n",
    "                        subplot_kw={'projection': ccrs.PlateCarree()},\n",
    "                        figsize=(16,8))\n",
    "\n",
    "# axs is a 2 dimensional array of `GeoAxes`. Flatten it into a 1-D array\n",
    "axs=axs.flatten()\n",
    "\n",
    "# Make the plots\n",
    "\n",
    "ax=axs[0]\n",
    "ax.set_facecolor('white')  # For consistency with Harris et. al 2022\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=1)\n",
    "ax.add_feature(cfeature.RIVERS, linewidth=1,edgecolor='black')\n",
    "ax.add_feature(cfeature.LAKES, linewidth=1,linestyle='-',edgecolor='black',facecolor='none')\n",
    "c = ax.pcolormesh(longitude, latitude, np.mean(IFS_variogram_all[:,0,:,:], axis=0), \n",
    "                  # norm=colors.LogNorm(*value_range_precip),\n",
    "                  vmin=0, vmax=0.7,\n",
    "                  transform=ccrs.PlateCarree(), cmap='YlGnBu')\n",
    "ax.set_title(f\"IFS variogram\",size=18)\n",
    "\n",
    "ax=axs[1]\n",
    "ax.set_facecolor('white')  # For consistency with Harris et. al 2022\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=1)\n",
    "ax.add_feature(cfeature.RIVERS, linewidth=1,edgecolor='black')\n",
    "ax.add_feature(cfeature.LAKES, linewidth=1,linestyle='-',edgecolor='black',facecolor='none')\n",
    "c = ax.pcolormesh(longitude, latitude, np.mean(cGAN_variogram_all[:,0,:,:], axis=0), \n",
    "                  # norm=colors.LogNorm(*value_range_precip),\n",
    "                  vmin=0, vmax=0.7,\n",
    "                  transform=ccrs.PlateCarree(), cmap='YlGnBu')\n",
    "ax.set_title(\"cGAN variogram\",size=18)\n",
    "\n",
    "# Adjust the location of the subplots on the page to make room for the colorbar\n",
    "fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.8,\n",
    "                    wspace=0.02, hspace=0.02)\n",
    "\n",
    "# Add a colorbar axis at the bottom of the graph\n",
    "cbar_ax = fig.add_axes([0.81, 0.275, 0.0125, 0.45])\n",
    "\n",
    "# Draw the colorbar\n",
    "cb=fig.colorbar(c, cax=cbar_ax,orientation='vertical')\n",
    "cb.ax.tick_params(labelsize=18)\n",
    "cb.set_label('Rainfall (mm/h)',size=18)\n",
    "\n",
    "# Save the picture\n",
    "# plt.savefig(f\"{plot_dir}/test.png\", format=\"png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f6d72-5314-4cd6-90c1-eb378a24e906",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-23T18:53:27.903521Z",
     "iopub.status.idle": "2024-01-23T18:53:27.903684Z",
     "shell.execute_reply": "2024-01-23T18:53:27.903609Z",
     "shell.execute_reply.started": "2024-01-23T18:53:27.903602Z"
    }
   },
   "outputs": [],
   "source": [
    "# So this is a case study\n",
    "# Keep that for reporting\n",
    "\n",
    "plt.pcolormesh(IFS_variogram,vmin=0,vmax=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"IFS_variogram\")\n",
    "plt.show()\n",
    "\n",
    "plt.pcolormesh(cGAN_variogram,vmin=0,vmax=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"cGAN_variogram\")\n",
    "plt.show()\n",
    "\n",
    "plt.pcolormesh(IMERG_rain,vmin=0,vmax=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"IMERG_rain\")\n",
    "plt.show()\n",
    "\n",
    "plt.pcolormesh(np.mean(tp_IFS,axis=0),vmin=0,vmax=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"IFS rain mean\")\n",
    "plt.show()\n",
    "\n",
    "plt.pcolormesh(np.mean(tp_GAN,axis=0),vmin=0,vmax=5)\n",
    "plt.colorbar()\n",
    "plt.title(\"cGAN rain mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90032401-124b-4f06-abcc-4db7ef4e8c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
